{\rtf1\ansi\ansicpg936\cocoartf1671\cocoasubrtf500
{\fonttbl\f0\fnil\fcharset134 PingFangSC-Semibold;\f1\fnil\fcharset134 PingFangSC-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue233;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c93333;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl400\sa280\qc\partightenfactor0

\f0\b\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 2: Core methods\
\pard\pardeftab720\sl340\partightenfactor0

\f1\b0\fs24 \cf2 The objective of these topics is to give you an opportunity to get a deeper understanding of some of the ideas we have covered in the course. Note: these are\uc0\u160 
\f0\b "implement from scratch"
\f1\b0 projects, that is, you must not use any machine learning libraries, apart from utilities to read data from files. This means that, for example, if you are implementing in Python, you\uc0\u160 cannot use any code from scikit-learn (except for reading data from a file), although you can use NumPy or SciPy code to implement your algorithm.\
\pard\pardeftab720\sl340\sa120\partightenfactor0
\cf2 \
\
\
\pard\pardeftab720\sl340\sa240\partightenfactor0

\f0\b \cf2 Topic 2.2: Nearest Neighbour
\f1\b0 \

\f0\b Task description
\f1\b0 Implement\uc0\u160 k-Nearest Neighbour (kNN) for both classification and numeric prediction using both Manhattan (L1) and Euclidean (L2) distances.\
For classification, test your version of kNN for range of values of k on the standard UCI data set\uc0\u160 {\field{\*\fldinst{HYPERLINK "applewebdata://BEE2CA15-C72B-4DED-BB7E-31B406D3BC25/data/ionosphere.arff"}}{\fldrslt \cf3 \ul \ulc3 \strokec3 ionosphere}}. Evaluate your system by leave-one-out cross-validation.\
For numeric prediction, test your version of kNN for range of values of k on the standard UCI data set\uc0\u160 {\field{\*\fldinst{HYPERLINK "applewebdata://BEE2CA15-C72B-4DED-BB7E-31B406D3BC25/data/autos.arff"}}{\fldrslt \cf3 \ul \ulc3 \strokec3 autos}}. For this data set you can remove examples with missing attribute values, which will reduce the number of examples to 159. The task is to predict the price given the 14 continuous and 1 integer attributes. You can also experiment with encoding the other attributes to allow their use in the distance function to see if this affects predictive accuracy. Evaluate your system by leave-one-out cross-validation.\
Next you will extend your methods to implement distance-weighted Nearest Neighbour (WNN). Rerun the algorithm for both the classification and numeric prediction tasks. Compare the results to analyse what difference (if any) distance-weighting has made to the predictive performance.\
Finally you must construct a probabilistic two-class classification\uc0\u160 target function\u160 for which you know the class probabilities. Calculate the Bayes error rate (refer to the "Classification" lectures to review this if you are not sure). Next, generate a dataset, with about the same number of features and examples as the UCI datasets you used earlier, using this target function. Now run your implementation of kNN and WNN on this dataset. Experiment with the parameter settings for this algorithm and determine how close to the Bayes error rate is the predictive performance. Is this what you would have expected ? Justify your answer.\

\f0\b Team
\f1\b0 \uc0\u160 2 or 3 person team\

\f0\b Difficulty
\f1\b0 \uc0\u160 3/5\
}